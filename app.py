import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import zhipuai
import requests
import google.generativeai as genai
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from pyvis.network import Network
import streamlit.components.v1 as components
import os
from streamlit_searchbox import st_searchbox
import re
import pycountry
import warnings
import torch
from transformers import pipeline

warnings.filterwarnings("ignore")

# --- é¡µé¢åŸºç¡€è®¾ç½® ---
st.set_page_config(
    page_title="IMDBç”µå½±æ•°æ®åˆ†æ",
    page_icon="âœ¨",
    layout="wide"
)

import platform

system = platform.system()
if system == "Windows":
    plt.rcParams['font.sans-serif'] = ['SimHei']
if system == "Darwin":  # macOS
    plt.rcParams['font.sans-serif'] = ['STHeiti']
plt.rcParams['axes.unicode_minus'] = False

# --- èµ„æºåŠ è½½ä¸ç¼“å­˜ ---
@st.cache_resource
def load_sentiment_pipeline():
    model_name = "./distilbert-imdb-finetuned/distilbert-imdb-finetuned"
    if torch.cuda.is_available():
        DEVICE = torch.device('cuda')
    elif torch.backends.mps.is_available():
        DEVICE = torch.device('mps')
    else:
        DEVICE = torch.device('cpu')
    return pipeline("sentiment-analysis", model=model_name, tokenizer=model_name, device=DEVICE)

# --- æ–°å¢: AI æ¨¡å‹é€‰æ‹©UI ---
st.sidebar.header("ğŸ¤– AI æ¨¡å‹è®¾ç½®")
# ä½¿ç”¨ session_state æ¥æŒä¹…åŒ–æ¨¡å‹é€‰æ‹©
if 'selected_model' not in st.session_state:
    st.session_state.selected_model = "Gemini (éœ€ä»£ç†)"

st.session_state.selected_model = st.sidebar.selectbox(
    "é€‰æ‹©æ‚¨æƒ³ä½¿ç”¨çš„AIæ¨¡å‹:",
    ["Gemini (éœ€ä»£ç†)", "æ™ºè°±AI (å›½å†…)", "æœ¬åœ°Ollama"],
    index=["Gemini (éœ€ä»£ç†)", "æ™ºè°±AI (å›½å†…)", "æœ¬åœ°Ollama"].index(st.session_state.selected_model)
)


gemini_is_configured = False
zhipuai_is_configured = False

try:
    if "GEMINI_API_KEY" in st.secrets and st.secrets["GEMINI_API_KEY"]:
        genai.configure(api_key=st.secrets["GEMINI_API_KEY"])
        gemini_is_configured = True
except (KeyError, AttributeError):
    pass # å…è®¸åœ¨æ²¡æœ‰Geminiå¯†é’¥çš„æƒ…å†µä¸‹è¿è¡Œ

try:
    if "ZHIPU_API_KEY" in st.secrets and st.secrets["ZHIPU_API_KEY"]:
        # ä»…æ£€æŸ¥å¯†é’¥å­˜åœ¨æ€§ï¼Œå®¢æˆ·ç«¯åœ¨è°ƒç”¨æ—¶å®ä¾‹åŒ–
        zhipuai_is_configured = True
except (KeyError, AttributeError):
    pass


if st.session_state.selected_model == "Gemini (éœ€ä»£ç†)" and not gemini_is_configured:
    st.sidebar.warning("Gemini APIå¯†é’¥æœªé…ç½®ã€‚è¯·åœ¨ .streamlit/secrets.toml æ–‡ä»¶ä¸­è®¾ç½® `GEMINI_API_KEY`ã€‚")
elif st.session_state.selected_model == "æ™ºè°±AI (å›½å†…)" and not zhipuai_is_configured:
    st.sidebar.warning("æ™ºè°±AI APIå¯†é’¥æœªé…ç½®ã€‚è¯·åœ¨ .streamlit/secrets.toml æ–‡ä»¶ä¸­è®¾ç½® `ZHIPU_API_KEY`ã€‚")
elif st.session_state.selected_model == "æœ¬åœ°Ollama":
    st.sidebar.info("è¯·ç¡®ä¿æ‚¨çš„æœ¬åœ°OllamaæœåŠ¡æ­£åœ¨è¿è¡Œã€‚æ¨èæ¨¡å‹: `llama3` æˆ– `qwen:7b`ã€‚")



@st.cache_data(show_spinner="ğŸ§  AI æ­£åœ¨æ€è€ƒ...")
def call_llm(prompt, model_selection):
    """æ ¹æ®é€‰æ‹©è°ƒç”¨ä¸åŒçš„å¤§è¯­è¨€æ¨¡å‹å¹¶è¿”å›ç»“æœ"""
    if model_selection == "Gemini (éœ€ä»£ç†)":
        if not gemini_is_configured:
            return "é”™è¯¯ï¼šGemini APIå¯†é’¥æœªé…ç½®ã€‚"
        try:
            model = genai.GenerativeModel("gemini-2.5-flash")
            response = model.generate_content(prompt)
            return response.text
        except Exception as e:
            return f"è°ƒç”¨Geminiæ—¶å‡ºé”™: {e}"

    elif model_selection == "æ™ºè°±AI (å›½å†…)":
        if not zhipuai_is_configured:
            return "é”™è¯¯ï¼šæ™ºè°±AI APIå¯†é’¥æœªé…ç½®ã€‚"
        try:
            client = zhipuai.ZhipuAI(api_key=st.secrets["ZHIPU_API_KEY"])
            response = client.chat.completions.create(
                model="glm-4-flash",
                messages=[{"role": "user", "content": prompt}],
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"è°ƒç”¨æ™ºè°±AIæ—¶å‡ºé”™: {e}"

    elif model_selection == "æœ¬åœ°Ollama":
        try:
            # å‡è®¾OllamaæœåŠ¡åœ¨æ ‡å‡†ç«¯å£è¿è¡Œ
            url = "http://localhost:11434/api/generate"
            payload = {
                "model": "llama3",
                "prompt": prompt,
                "stream": False
            }
            response = requests.post(url, json=payload, timeout=120) # å¢åŠ è¶…æ—¶æ—¶é—´
            response.raise_for_status() # å¦‚æœè¯·æ±‚å¤±è´¥åˆ™æŠ›å‡ºå¼‚å¸¸
            return response.json()['response']
        except requests.exceptions.ConnectionError:
            return "é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ°æœ¬åœ°OllamaæœåŠ¡ã€‚è¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œã€‚"
        except Exception as e:
            return f"è°ƒç”¨æœ¬åœ°Ollamaæ—¶å‡ºé”™: {e}"
    else:
        return "é”™è¯¯ï¼šæ— æ•ˆçš„AIæ¨¡å‹é€‰æ‹©ã€‚"


# --- èµ„æºåŠ è½½ä¸ç¼“å­˜ ---
@st.cache_data
def convert_alpha2_to_alpha3(alpha_2):
    if alpha_2 == 'UK': return 'GBR'
    if alpha_2 == 'SU': return 'RUS'
    try:
        return pycountry.countries.get(alpha_2=alpha_2).alpha_3
    except (AttributeError, LookupError):
        return None


@st.cache_data
def load_all_data():
    base_path = "data_possessed"
    try:
        print("Loading and concatenating chunked files for reviews and career data...")

        # åŠ è½½å¹¶åˆå¹¶ reviews_cleaned (2ä¸ªéƒ¨åˆ†)
        reviews_parts = [pd.read_parquet(f"{base_path}/reviews_cleaned_part_{i}.parquet") for i in range(1, 3)]
        reviews_df = pd.concat(reviews_parts, ignore_index=True)

        # åŠ è½½å¹¶åˆå¹¶ career_data (3ä¸ªéƒ¨åˆ†)
        career_parts = [pd.read_parquet(f"{base_path}/career_data_part_{i}.parquet") for i in range(1, 5)]
        career_df = pd.concat(career_parts, ignore_index=True)

        print("All data loaded successfully.")

        data = {
            'metadata': pd.read_parquet(f"{base_path}/movies_metadata.parquet"),
            'reviews': reviews_df,
            'names': pd.read_parquet(f"{base_path}/names.parquet").set_index('nconst'),
            'career': career_df,
            'akas': pd.read_parquet(f"{base_path}/movie_akas.parquet"),
            'collab': pd.read_parquet(f"{base_path}/collaboration_counts.parquet"),
        }
        data['names'] = data['names'].sort_values('primaryName')
        return data
    except FileNotFoundError as e:
        st.error(
            f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ã€‚è¯·ç¡®ä¿æ‚¨å·²æˆåŠŸè¿è¡Œ `prepare_data.py` è„šæœ¬æ¥ç”Ÿæˆæ‰€æœ‰å¿…éœ€çš„æ–‡ä»¶ã€‚å…·ä½“æ–‡ä»¶: {e.filename}")
        st.info(
            "å¦‚æœæ‚¨åˆšåˆšä¿®æ”¹äº†è„šæœ¬ä»¥æ‹†åˆ†æ–‡ä»¶ï¼Œè¯·å…ˆåˆ é™¤æ—§çš„ `career_data.parquet` å’Œ `reviews_cleaned.parquet` æ–‡ä»¶ï¼Œç„¶åé‡æ–°è¿è¡Œ `prepare_data.py`ã€‚")
        return None


# --- åŠ è½½èµ„æº ---
with st.spinner('æ­£åœ¨åŠ è½½æµ·é‡ç”µå½±æ•°æ®...'):
    data = load_all_data()

if data is None: st.stop()

# --- ä¸»ç•Œé¢ ---
st.title("âœ¨ IMDBç”µå½±æ•°æ®å¯è§†åŒ–å¹³å°")
st.markdown("æ¬¢è¿æ¥åˆ°IMDBæ•°æ®æ¢ç´¢å¹³å°ï¼æœ¬å¹³å°èåˆ**ä¼ ç»Ÿæ•°æ®åˆ†æ**ä¸**ç”Ÿæˆå¼AI**ï¼Œä¸ºæ‚¨æä¾›æ›´æ·±å±‚æ¬¡çš„ç”µå½±è¡Œä¸šæ´å¯Ÿã€‚")

with st.expander("â„¹ï¸ ç‚¹å‡»æŸ¥çœ‹æ•°æ®é›†æ€»è§ˆ"):
    st.markdown(f"""
    - **ğŸ¬ ç”µå½±å…ƒæ•°æ®**: åŒ…å« **{len(data['metadata']):,}** éƒ¨ç”µå½±ä¿¡æ¯ã€‚
    - **âœï¸ å½±è¯„æ•°æ®**: åŒ…å« **{len(data['reviews']):,}** æ¡ç”¨æˆ·è¯„è®ºã€‚
    - **ğŸ§‘â€ğŸ¨ å½±äººæ•°æ®**: åŒ…å« **{len(data['names']):,}** ä½æ ¸å¿ƒå½±äººä¿¡æ¯ã€‚
    - **ğŸŒ å…¨çƒä¸Šæ˜ æ•°æ®**: è®°å½•é«˜ç¥¨ç”µå½±çš„å…¨çƒä¸Šæ˜ æƒ…å†µã€‚
    """)
    st.markdown(f"**ğŸ¤– å½“å‰AIåˆ†ææ¨¡å‹**: `{st.session_state.selected_model}` (å¯åœ¨å·¦ä¾§è¾¹æ åˆ‡æ¢)")

# --- Tabå¸ƒå±€ ---
tab1, tab2, tab3, tab4, tab5 = st.tabs(
    ["ğŸ“Š ç”µå½±å…ƒæ•°æ®æ¢ç´¢", "ğŸŒ å…¨çƒå¸‚åœºåˆ†æ", "ğŸ‘¥ è¡Œä¸šç½‘ç»œä¸äººç‰©åˆ†æ", "ğŸ“ å½±è¯„æ–‡æœ¬åˆ†æ",
     "ğŸ¤– å½±è¯„æ™ºèƒ½å‰–æ"])

# --- Tab 1: ç”µå½±å…ƒæ•°æ®æ¢ç´¢ ---
with tab1:
    st.header("ğŸ“Š ç”µå½±å…ƒæ•°æ®æ¢ç´¢")
    st.sidebar.header("å…ƒæ•°æ®ç­›é€‰å™¨")
    min_year, max_year = int(data['metadata']['startYear'].min()), int(data['metadata']['startYear'].max())
    year_range = st.sidebar.slider("é€‰æ‹©ä¸Šæ˜ å¹´ä»½èŒƒå›´:", min_year, max_year, (min_year, max_year), key="tab1_year")
    all_genres = sorted(list(set(g for sublist in data['metadata']['genres_list'] for g in sublist)))
    selected_genres = st.sidebar.multiselect("é€‰æ‹©ç”µå½±ç±»å‹:", options=all_genres, default=["Action", "Drama", "Comedy", "Family", "Crime", "Documentary", "Adventure", "Fantasy", "Thriller", "Horror"],
                                             key="tab1_genres")
    min_votes = st.sidebar.slider("æœ€å°æŠ•ç¥¨æ•°:", 0, 50000, 1000, step=100, key="tab1_votes")

    filtered_meta_df = data['metadata'][
        (data['metadata']['startYear'] >= year_range[0]) & (data['metadata']['startYear'] <= year_range[1]) &
        (data['metadata']['numVotes'] >= min_votes)
        ]
    if selected_genres:
        selected_genres_set = set(selected_genres)
        filtered_meta_df = filtered_meta_df[
            filtered_meta_df['genres_list'].apply(lambda g: not selected_genres_set.isdisjoint(g))]

    if filtered_meta_df.empty:
        st.warning("åœ¨å½“å‰ç­›é€‰æ¡ä»¶ä¸‹ï¼Œæ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç”µå½±ã€‚")
    else:
        st.subheader("ğŸ“ˆ æ ¸å¿ƒæŒ‡æ ‡ä¸€è§ˆ")
        kpi1, kpi2, kpi3 = st.columns(3)
        kpi1.metric("ç”µå½±æ€»æ•°", f"{len(filtered_meta_df):,}")
        kpi2.metric("å¹³å‡IMDBè¯„åˆ†", f"{filtered_meta_df['averageRating'].mean():.2f}")
        kpi3.metric("æ€»æŠ•ç¥¨æ•°", f"{filtered_meta_df['numVotes'].sum():,}")
        st.markdown("---")
        st.subheader("ğŸ¨ å¯è§†åŒ–åˆ†æ")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("##### ğŸ­ å„ç±»å‹ç”µå½±æ•°é‡")
            genre_df = filtered_meta_df.explode('genres_list').rename(columns={'genres_list': 'genre'})
            genre_counts = genre_df[genre_df['genre'].isin(selected_genres)]['genre'].value_counts()
            fig_genre_count = px.bar(genre_counts, x=genre_counts.index, y=genre_counts.values,
                                     labels={'x': 'ç”µå½±ç±»å‹', 'y': 'æ•°é‡'}, color=genre_counts.index,
                                     color_discrete_sequence=px.colors.qualitative.Pastel)
            st.plotly_chart(fig_genre_count, use_container_width=True)
            st.markdown("##### â­ è¯„åˆ† vs. æŠ•ç¥¨æ•° (å¯¹æ•°å°ºåº¦)")
            fig_scatter_votes = px.scatter(filtered_meta_df.sample(min(1000, len(filtered_meta_df))), x="averageRating",
                                           y="numVotes", hover_name="primaryTitle", log_y=True,
                                           labels={"averageRating": "IMDBè¯„åˆ†", "numVotes": "æŠ•ç¥¨æ•° (å¯¹æ•°)"})
            st.plotly_chart(fig_scatter_votes, use_container_width=True)
        with col2:
            st.markdown("##### ğŸ“ˆ å¹³å‡è¯„åˆ†éšå¹´ä»½å˜åŒ–")
            rating_over_time = filtered_meta_df.groupby('startYear')['averageRating'].mean().reset_index()
            fig_rating_time = px.line(rating_over_time, x='startYear', y='averageRating',
                                      labels={'startYear': 'ä¸Šæ˜ å¹´ä»½', 'averageRating': 'å¹³å‡IMDBè¯„åˆ†'})
            st.plotly_chart(fig_rating_time, use_container_width=True)
            st.markdown("##### ğŸ“‹ ç­›é€‰ç»“æœè¯¦æƒ… (Top 10)")
            st.dataframe(filtered_meta_df[['primaryTitle', 'startYear', 'averageRating', 'numVotes']].rename(
                columns={'primaryTitle': 'æ ‡é¢˜', 'startYear': 'å¹´ä»½', 'averageRating': 'è¯„åˆ†',
                         'numVotes': 'æŠ•ç¥¨æ•°'}).head(10))

# --- Tab 2: å…¨çƒå¸‚åœºåˆ†æ ---
with tab2:
    st.header("ğŸŒ ç”µå½±å…¨çƒåŒ–å¸‚åœºåˆ†æ")
    st.markdown("ä»å®è§‚è§†è§’åˆ†æ**æŸä¸€ç±»ç”µå½±**åœ¨å…¨çƒçš„å¸‚åœºåˆ†å¸ƒçƒ­åº¦ï¼Œå¹¶å¯ä¸‹é’»æŸ¥çœ‹å…·ä½“å›½å®¶/åœ°åŒºçš„ä¸Šæ˜ å½±ç‰‡ã€‚")
    st.sidebar.header("å…¨çƒåŒ–ç­›é€‰å™¨")
    min_year_t2, max_year_t2 = int(data['metadata']['startYear'].min()), int(data['metadata']['startYear'].max())
    year_range_t2 = st.sidebar.slider("é€‰æ‹©ä¸Šæ˜ å¹´ä»½èŒƒå›´:", min_year_t2, max_year_t2, (min_year_t2, max_year_t2),
                                      key="tab2_year")
    all_genres_t2 = sorted(list(set(g for sublist in data['metadata']['genres_list'] for g in sublist)))
    selected_genres_t2 = st.sidebar.multiselect("é€‰æ‹©ç”µå½±ç±»å‹:", options=all_genres_t2, default=["Action", "Drama", "Comedy", "Family", "Crime", "Documentary", "Adventure", "Fantasy", "Thriller", "Horror"],
                                                key="tab2_genres")
    min_votes_t2 = st.sidebar.slider("æœ€å°æŠ•ç¥¨æ•°:", 1000, 100000, 25000, step=1000, key="tab2_votes")

    filtered_movies_map = data['metadata'][
        (data['metadata']['startYear'] >= year_range_t2[0]) & (data['metadata']['startYear'] <= year_range_t2[1]) &
        (data['metadata']['numVotes'] >= min_votes_t2)
        ]
    if selected_genres_t2:
        selected_genres_set_t2 = set(selected_genres_t2)
        filtered_movies_map = filtered_movies_map[
            filtered_movies_map['genres_list'].apply(lambda g: not selected_genres_set_t2.isdisjoint(g))
        ]

    if filtered_movies_map.empty:
        st.warning("åœ¨å½“å‰ç­›é€‰æ¡ä»¶ä¸‹ï¼Œæ²¡æœ‰æ‰¾åˆ°ä»»ä½•ç”µå½±ã€‚")
    else:
        st.info(f"æ ¹æ®æ‚¨çš„ç­›é€‰ï¼Œå…±æ‰¾åˆ° **{len(filtered_movies_map):,}** éƒ¨ç”µå½±è¿›è¡Œå…¨çƒå¸‚åœºåˆ†æã€‚")
        target_tconsts = filtered_movies_map['tconst']
        map_akas_df = data['akas'][data['akas']['tconst'].isin(target_tconsts)]
        if map_akas_df.empty:
            st.warning("è¿™äº›ç”µå½±æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å…¨çƒä¸Šæ˜ ä¿¡æ¯ã€‚")
        else:
            region_summary = map_akas_df.groupby('region')['tconst'].nunique().reset_index(
                name='movie_count').sort_values('movie_count', ascending=False)
            region_summary['iso_alpha'] = region_summary['region'].apply(convert_alpha2_to_alpha3)
            region_summary_mapped = region_summary.dropna(subset=['iso_alpha'])
            if region_summary_mapped.empty:
                st.warning("æ— æ³•å°†ç­›é€‰å‡ºçš„ç”µå½±åœ°åŒºä»£ç æ˜ å°„åˆ°æ ‡å‡†åœ°ç†ç¼–ç ï¼Œå› æ­¤æ— æ³•ç”Ÿæˆåœ°å›¾ã€‚")
            else:
                st.subheader("ğŸ—ºï¸ å…¨çƒå¸‚åœºåˆ†å¸ƒçƒ­åŠ›å›¾")
                fig_map = go.Figure(data=go.Choropleth(
                    locations=region_summary_mapped['iso_alpha'],
                    z=region_summary_mapped['movie_count'],
                    text=region_summary_mapped['region'],
                    colorscale=px.colors.sequential.Plasma, reversescale=True,
                    marker_line_color='darkgray', marker_line_width=0.5,
                    colorbar_title='ç”µå½±æ•°é‡',
                    hovertemplate='<b>%{text}</b><br>ç”µå½±æ•°é‡: %{z}<extra></extra>'
                ))

                fig_map.update_layout(
                    title_text=f'ç­›é€‰å‡ºçš„{len(filtered_movies_map):,}éƒ¨ç”µå½±çš„å…¨çƒå¸‚åœºåˆ†å¸ƒ',
                    geo=dict(
                        showframe=False,
                        showcoastlines=False,
                        projection_type='natural earth',
                        lataxis_showgrid=True,
                        lonaxis_showgrid=True
                    ),
                    margin=dict(l=0, r=0, t=40, b=0),
                    height=600
                )

                st.plotly_chart(fig_map, use_container_width=True)

            st.markdown("---")
            st.subheader("ğŸ“ ä¸‹é’»åˆ†æï¼šæŸ¥çœ‹å…·ä½“åœ°åŒºçš„ä¸Šæ˜ å½±ç‰‡")
            col1, col2 = st.columns([1, 2])
            with col1:
                st.dataframe(region_summary[['region', 'movie_count']].rename(
                    columns={'region': 'åœ°åŒº', 'movie_count': 'ç”µå½±æ•°é‡'}))
            with col2:
                available_regions = region_summary['region'].unique()
                if len(available_regions) > 0:
                    selected_region = st.selectbox("é€‰æ‹©ä¸€ä¸ªåœ°åŒºæŸ¥çœ‹è¯¦æƒ…:", options=available_regions)
                    if selected_region:
                        region_movie_tconsts = map_akas_df[map_akas_df['region'] == selected_region]['tconst'].unique()
                        region_movies_details = filtered_movies_map[
                            filtered_movies_map['tconst'].isin(region_movie_tconsts)]
                        region_akas = data['akas'][(data['akas']['tconst'].isin(region_movies_details['tconst'])) & (
                                    data['akas']['region'] == selected_region)]
                        final_df = pd.merge(
                            region_movies_details[['tconst', 'primaryTitle', 'startYear', 'averageRating']],
                            region_akas[['tconst', 'title']].drop_duplicates(subset=['tconst']), on='tconst',
                            how='left'
                        )

                        search_query = st.text_input(
                            f"åœ¨ **{selected_region}** çš„ç»“æœä¸­æŒ‰æ ‡é¢˜æœç´¢ ğŸ”",
                            placeholder="ä¾‹å¦‚ï¼šDark Knight, Inception..."
                        )

                        display_df = final_df.rename(
                            columns={'primaryTitle': 'åŸå§‹æ ‡é¢˜', 'title': 'å½“åœ°è¯‘å', 'startYear': 'å¹´ä»½',
                                     'averageRating': 'è¯„åˆ†'}
                        )

                        if search_query:
                            mask = display_df['åŸå§‹æ ‡é¢˜'].str.contains(search_query, case=False, na=False) | \
                                   display_df['å½“åœ°è¯‘å'].str.contains(search_query, case=False, na=False)
                            filtered_display_df = display_df[mask]
                        else:
                            filtered_display_df = display_df

                        st.write(f"åœ¨ **{selected_region}** å…±æ‰¾åˆ° **{len(filtered_display_df)}** éƒ¨ç›¸å…³ç”µå½±:")
                        st.dataframe(
                            filtered_display_df[['åŸå§‹æ ‡é¢˜', 'å½“åœ°è¯‘å', 'å¹´ä»½', 'è¯„åˆ†']],
                            use_container_width=True
                        )

# --- Tab 3: è¡Œä¸šç½‘ç»œä¸äººç‰©åˆ†æ ---
with tab3:
    st.header("ğŸ‘¥ è¡Œä¸šç½‘ç»œä¸äººç‰©åˆ†æ")
    st.markdown("é€šè¿‡è®¾ç½®**æœ€å°ä½œå“æ•°**ç­›é€‰æ ¸å¿ƒå½±äººï¼Œå†åˆ©ç”¨**æ¨¡ç³Šæœç´¢**å¿«é€Ÿå®šä½å¹¶åˆ†æå…¶èŒä¸šç”Ÿæ¶¯ä¸åˆä½œç½‘ç»œã€‚")
    min_movie_count = 15

    person_list_df = data['names'].reset_index()
    person_list_df.dropna(subset=['primaryName', 'nconst', 'movie_count'], inplace=True)
    filtered_person_df = person_list_df[person_list_df['movie_count'] >= min_movie_count]

    def search_persons(search_term: str):
        if not search_term:
            recommendations = filtered_person_df.nlargest(5, 'movie_count')
            return (recommendations['primaryName'] + " (" + recommendations['nconst'] + ")").tolist()
        lower_search_term = search_term.lower()
        results = filtered_person_df[filtered_person_df['primaryName'].str.lower().str.contains(lower_search_term)]
        return (results['primaryName'] + " (" + results['nconst'] + ")").tolist()

    st.markdown(f"##### ğŸ” æœç´¢å½±äºº (å½“å‰å·²ç­›é€‰å‡º {len(filtered_person_df):,} ä½ä½œå“æ•°ä¸ä½äº {min_movie_count} çš„å½±äºº)")
    selected_display_name = st_searchbox(search_function=search_persons,
                                         placeholder="è¾“å…¥'spielberg' è¯•è¯•",
                                         label="æœç´¢ä¸€ä½æ¼”å‘˜æˆ–å¯¼æ¼”:", key="person_searchbox_filtered",
                                         default="Steven Spielberg (nm0000229)")

    if selected_display_name:
        match = re.search(r'\((\w+)\)$', selected_display_name)
        if not match:
            st.warning("è¯·ä»æœç´¢ç»“æœä¸­é€‰æ‹©ä¸€ä¸ªæœ‰æ•ˆçš„å½±äººæ¡ç›®ã€‚")
        else:
            person_nconst, selected_person_name = match.group(1), selected_display_name.rsplit(' (', 1)[0]
            person_info = person_list_df[person_list_df['nconst'] == person_nconst].iloc[0]
            st.subheader(f"ğŸï¸ {selected_person_name} çš„åˆ†ææŠ¥å‘Š")
            st.metric("æ”¶å½•ä½œå“æ€»æ•°", f"{int(person_info['movie_count'])} éƒ¨")

            st.markdown("#### ğŸš€ èŒä¸šç”Ÿæ¶¯è½¨è¿¹")
            with st.spinner(f"æ­£åœ¨æŸ¥è¯¢ {selected_person_name} çš„èŒä¸šç”Ÿæ¶¯æ•°æ®..."):
                career_df = data['career'][data['career']['nconst'] == person_nconst]
                if not career_df.empty:
                    fig_career = px.scatter(career_df, x='startYear', y='averageRating', size='numVotes',
                                            color='numVotes', hover_name='primaryTitle',
                                            title="ä½œå“è¯„åˆ†ä¸å½±å“åŠ›éšå¹´ä»½å˜åŒ–",
                                            labels={'startYear': 'å¹´ä»½', 'averageRating': 'IMDBè¯„åˆ†',
                                                    'numVotes': 'æŠ•ç¥¨æ•°'},
                                            color_continuous_scale=px.colors.sequential.Viridis)
                    st.plotly_chart(fig_career, use_container_width=True)
                else:
                    st.info(f"æœªåœ¨æ•°æ®é›†ä¸­æ‰¾åˆ° {selected_person_name} çš„èŒä¸šç”Ÿæ¶¯æ•°æ®ã€‚")

            st.markdown("#### ğŸ¤ åˆä½œç½‘ç»œ")
            with st.spinner(f"æ­£åœ¨æŸ¥è¯¢ {selected_person_name} çš„åˆä½œæ•°æ®..."):
                part1, part2 = data['collab'][data['collab']['person1'] == person_nconst], data['collab'][
                    data['collab']['person2'] == person_nconst]
                if not part1.empty: part1 = part1.rename(columns={'person2': 'collaborator_nconst'})
                if not part2.empty: part2 = part2.rename(columns={'person1': 'collaborator_nconst'})
                all_collabs = pd.concat([part1, part2])
                if not all_collabs.empty:
                    all_collabs['collaborator_name'] = all_collabs['collaborator_nconst'].map(
                        data['names']['primaryName'])
                    all_collabs.dropna(subset=['collaborator_name'], inplace=True)
                    top_collabs = all_collabs.groupby('collaborator_name')['count'].sum().nlargest(15)
                    col1, col2 = st.columns([1, 1])
                    with col1:
                        st.markdown("##### ğŸ† åˆä½œæœ€é¢‘ç¹çš„å½±äºº")
                        fig_collab = px.bar(top_collabs, x=top_collabs.values, y=top_collabs.index, orientation='h',
                                            labels={'x': 'åˆä½œæ¬¡æ•°', 'y': 'å½±äººå§“å'},
                                            color=top_collabs.values,
                                            color_continuous_scale=px.colors.sequential.Teal)
                        fig_collab.update_layout(yaxis={'categoryorder': 'total ascending'})
                        st.plotly_chart(fig_collab, use_container_width=True)
                    with col2:
                        st.markdown("##### ğŸŒ äº¤äº’å¼åˆä½œç½‘ç»œ")
                        st.markdown(
                            "<small>ç½‘ç»œå›¾å±•ç¤ºäº†ä¸ä¸­å¿ƒäººç‰©åˆä½œæœ€é¢‘ç¹çš„å½±äººã€‚<b>èŠ‚ç‚¹å¤§å°</b>å’Œ<b>è¿çº¿ç²—ç»†</b>ä»£è¡¨åˆä½œæ¬¡æ•°çš„å¤šå°‘ã€‚æ‚¨å¯ä»¥ä½¿ç”¨å›¾ä¸‹æ–¹çš„æ§ä»¶è°ƒæ•´å¸ƒå±€ã€‚</small>",
                            unsafe_allow_html=True
                        )

                        net = Network(height="450px", width="100%", notebook=True, directed=False,
                                      cdn_resources='in_line')
                        net.barnes_hut(gravity=-3000, central_gravity=0.25, spring_length=200, spring_strength=0.05,
                                       damping=0.09)
                        central_node_title = f"""
                        <b>{selected_person_name}</b><br>
                        (ä¸­å¿ƒäººç‰©)<br>
                        æ”¶å½•ä½œå“: {int(person_info['movie_count'])} éƒ¨
                        """
                        net.add_node(selected_person_name, label=selected_person_name, color="#FF4B4B", size=30,
                                     title=central_node_title, font={'size': 20})

                        for name, count in top_collabs.items():
                            node_size = 12 + count * 2.5
                            collaborator_title = f"""
                            <b>{name}</b><br>
                            ä¸ {selected_person_name} åˆä½œ: {count} æ¬¡
                            """
                            net.add_node(name, label=name, size=node_size, title=collaborator_title,
                                         color="#66CDAA")
                            edge_title = f"åˆä½œ {count} æ¬¡"
                            net.add_edge(selected_person_name, name, value=count, title=edge_title)
                        # net.show_buttons(filter_=['physics'])

                        try:
                            net.save_graph("network.html")
                            with open("network.html", "r", encoding="utf-8") as f:
                                html_code = f.read()
                            components.html(html_code, height=500)
                        finally:
                            if os.path.exists("network.html"):
                                os.remove("network.html")
                else:
                    st.info(f"æœªåœ¨Topç”µå½±çš„åˆä½œå…³ç³»æ•°æ®åº“ä¸­æ‰¾åˆ° {selected_person_name} çš„æ•°æ®ã€‚")
    else:
        st.info("â¬†ï¸ è¯·åœ¨ä¸Šé¢çš„æœç´¢æ¡†ä¸­è¾“å…¥å½±äººå§“åè¿›è¡Œæœç´¢å’Œé€‰æ‹©ã€‚")


# --- Tab 4: å½±è¯„æ–‡æœ¬åˆ†æ ---
from wordcloud import STOPWORDS
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

CUSTOM_STOPWORDS = {
    "movie", "movie", "film", "films", "one", "show", "see", "watch", "time", "story",
    "character", "characters", "really", "make", "even", "get", "like",
    "acting", "plot", "director"
}
WORDCLOUD_STOPWORDS = STOPWORDS.union(CUSTOM_STOPWORDS)
SKLEARN_STOPWORDS = ENGLISH_STOP_WORDS.union(CUSTOM_STOPWORDS)

@st.cache_data
def generate_wordcloud(text_series, title):
    sample_size = min(len(text_series.dropna()), 2000)
    text = ' '.join(review for review in text_series.dropna().sample(sample_size, random_state=42))
    if not text: return None
    wordcloud = WordCloud(
        width=800, height=400,
        background_color='white',
        collocations=False,
        stopwords=WORDCLOUD_STOPWORDS
    ).generate(text)
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wordcloud, interpolation='bilinear')
    ax.set_title(title, fontdict={'fontsize': 20})
    ax.axis('off')
    return fig


@st.cache_data
def get_top_ngrams(corpus, ngram_range=(2, 2), top_k=20):
    vec = CountVectorizer(
        ngram_range=ngram_range,
        stop_words=list(SKLEARN_STOPWORDS)
    ).fit(corpus.dropna())
    bag_of_words = vec.transform(corpus.dropna())
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)
    return pd.DataFrame(words_freq[:top_k], columns=['N-gram', 'æ•°é‡'])


with tab4:
    st.header("ğŸ“ å½±è¯„æ–‡æœ¬åˆ†æ")
    analysis_type = st.radio("é€‰æ‹©åˆ†æç±»å‹:", ["è¯äº‘å›¾", "é«˜é¢‘è¯ç»„ (N-grams)", f"ğŸ¤– {st.session_state.selected_model.split(' ')[0]} AI æ€»ç»“"], horizontal=True,
                             key="tab4_radio")

    positive_reviews = data['reviews'][data['reviews']['sentiment'] == 'positive']['review_cleaned']
    negative_reviews = data['reviews'][data['reviews']['sentiment'] == 'negative']['review_cleaned']

    if analysis_type == "è¯äº‘å›¾":
        st.markdown("é€šè¿‡è¯äº‘å›¾ï¼Œç›´è§‚å¯¹æ¯”æ­£é¢ä¸è´Ÿé¢è¯„è®ºä¸­çš„é«˜é¢‘è¯æ±‡ã€‚ï¼ˆå·²ç§»é™¤'movie', 'film'ç­‰é€šç”¨è¯ï¼‰")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("##### ğŸ‘ æ­£é¢è¯„è®ºé«˜é¢‘è¯äº‘")
            with st.spinner("æ­£åœ¨ç”Ÿæˆæ­£é¢è¯äº‘..."):
                fig_pos = generate_wordcloud(positive_reviews, "æ­£é¢è¯„è®ºé«˜é¢‘è¯")
                if fig_pos: st.pyplot(fig_pos)
                else: st.warning("æ— æ­£é¢è¯„è®ºæ•°æ®ã€‚")
        with col2:
            st.markdown("##### ğŸ‘ è´Ÿé¢è¯„è®ºé«˜é¢‘è¯äº‘")
            with st.spinner("æ­£åœ¨ç”Ÿæˆè´Ÿé¢è¯äº‘..."):
                fig_neg = generate_wordcloud(negative_reviews, "è´Ÿé¢è¯„è®ºé«˜é¢‘è¯")
                if fig_neg: st.pyplot(fig_neg)
                else: st.warning("æ— è´Ÿé¢è¯„è®ºæ•°æ®ã€‚")

    elif analysis_type == "é«˜é¢‘è¯ç»„ (N-grams)":
        st.markdown("é€šè¿‡åˆ†æé«˜é¢‘è¯ç»„ï¼ˆäºŒå…ƒç»„ï¼‰ï¼Œå‘ç°æ›´å…·æ„ä¹‰çš„è¡¨è¾¾æ–¹å¼ã€‚ï¼ˆå·²ç§»é™¤'movie', 'film'ç­‰é€šç”¨è¯ï¼‰")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("##### ğŸ‘ æ­£é¢è¯„è®ºé«˜é¢‘è¯ç»„")
            with st.spinner("æ­£åœ¨åˆ†ææ­£é¢è¯ç»„..."):
                pos_ngrams = get_top_ngrams(positive_reviews, ngram_range=(2, 2), top_k=15)
                fig = px.bar(pos_ngrams, x='æ•°é‡', y='N-gram', orientation='h', title="Top 15 æ­£é¢è¯„è®ºäºŒå…ƒç»„", color_discrete_sequence=px.colors.qualitative.Pastel)
                fig.update_layout(yaxis={'categoryorder': 'total ascending'})
                st.plotly_chart(fig, use_container_width=True)
        with col2:
            st.markdown("##### ğŸ‘ è´Ÿé¢è¯„è®ºé«˜é¢‘è¯ç»„")
            with st.spinner("æ­£åœ¨åˆ†æè´Ÿé¢è¯ç»„..."):
                neg_ngrams = get_top_ngrams(negative_reviews, ngram_range=(2, 2), top_k=15)
                fig = px.bar(neg_ngrams, x='æ•°é‡', y='N-gram', orientation='h', title="Top 15 è´Ÿé¢è¯„è®ºäºŒå…ƒç»„", color_discrete_sequence=px.colors.qualitative.Bold)
                fig.update_layout(yaxis={'categoryorder': 'total ascending'})
                st.plotly_chart(fig, use_container_width=True)

    elif analysis_type.endswith("AI æ€»ç»“"):
        current_model_name = st.session_state.selected_model.split(' ')[0]
        st.markdown(f"åˆ©ç”¨ **{current_model_name}** å¤§æ¨¡å‹ï¼Œä»æµ·é‡è¯„è®ºä¸­æç‚¼æ ¸å¿ƒè§‚ç‚¹ï¼Œæ´å¯Ÿè§‚ä¼—çš„çœŸå®æƒ³æ³•ã€‚")
        sample_size = 20
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("##### ğŸ‘ æ­£é¢è¯„è®ºä¸»é¢˜æ€»ç»“")
            with st.spinner(f"ğŸ¤– {current_model_name} æ­£åœ¨é˜…è¯»æ­£é¢è¯„è®ºå¹¶æç‚¼è§‚ç‚¹..."):
                pos_sample = "\n".join(positive_reviews.dropna().sample(min(len(positive_reviews.dropna()), sample_size), random_state=1))
                prompt = f"""
                ä½œä¸ºä¸€åä¸“ä¸šçš„ç”µå½±è¯„è®ºåˆ†æå¸ˆï¼Œè¯·æ ¹æ®ä»¥ä¸‹{sample_size}æ¡çœŸå®çš„æ­£é¢å½±è¯„ï¼Œæ€»ç»“å‡ºè§‚ä¼—ä»¬ä¸»è¦ç§°èµçš„3åˆ°5ä¸ªæ ¸å¿ƒä¸»é¢˜ï¼ˆä¾‹å¦‚ï¼šæ¼”å‘˜æ¼”æŠ€ã€å¯¼æ¼”é£æ ¼ã€å‰§æƒ…è½¬æŠ˜ã€è§†è§‰æ•ˆæœã€æƒ…æ„Ÿå…±é¸£ç­‰ï¼‰ã€‚
                å¯¹äºæ¯ä¸ªä¸»é¢˜ï¼Œè¯·ç”¨ä¸€å¥è¯æ¦‚æ‹¬ï¼Œå¹¶å¼•ç”¨1-2å¥æœ€èƒ½ä»£è¡¨è¯¥è§‚ç‚¹çš„åŸè¯ä½œä¸ºä¾‹å­ã€‚è¯·ä½¿ç”¨Markdownæ ¼å¼åŒ–ä½ çš„å›ç­”ï¼Œé‡ç‚¹åŠ ç²—ã€‚

                å¾…åˆ†æçš„æ­£é¢è¯„è®ºï¼š
                ---
                {pos_sample}
                ---
                """
                summary = call_llm(prompt, st.session_state.selected_model)
                st.markdown(summary)

        with col2:
            st.markdown("##### ğŸ‘ è´Ÿé¢è¯„è®ºä¸»é¢˜æ€»ç»“")
            with st.spinner(f"ğŸ¤– {current_model_name} æ­£åœ¨é˜…è¯»è´Ÿé¢è¯„è®ºå¹¶å¯»æ‰¾æ§½ç‚¹..."):
                neg_sample = "\n".join(negative_reviews.dropna().sample(min(len(negative_reviews.dropna()), sample_size), random_state=1))
                prompt = f"""
                ä½œä¸ºä¸€åä¸“ä¸šçš„ç”µå½±è¯„è®ºåˆ†æå¸ˆï¼Œè¯·æ ¹æ®ä»¥ä¸‹{sample_size}æ¡çœŸå®çš„è´Ÿé¢è¯„è®ºï¼Œæ€»ç»“å‡ºè§‚ä¼—ä»¬ä¸»è¦æŠ±æ€¨çš„3åˆ°5ä¸ªæ ¸å¿ƒé—®é¢˜ï¼ˆä¾‹å¦‚ï¼šå‰§æƒ…ç©ºæ´ã€è§’è‰²å¡‘é€ å¤±è´¥ã€èŠ‚å¥æ‹–æ²“ã€ç‰¹æ•ˆå»‰ä»·ç­‰ï¼‰ã€‚
                å¯¹äºæ¯ä¸ªé—®é¢˜ï¼Œè¯·ç”¨ä¸€å¥è¯æ¦‚æ‹¬ï¼Œå¹¶å¼•ç”¨1-2å¥æœ€èƒ½ä»£è¡¨è¯¥è§‚ç‚¹çš„åŸè¯ä½œä¸ºä¾‹å­ã€‚è¯·ä½¿ç”¨Markdownæ ¼å¼åŒ–ä½ çš„å›ç­”ï¼Œé‡ç‚¹åŠ ç²—ã€‚

                å¾…åˆ†æçš„è´Ÿé¢è¯„è®ºï¼š
                ---
                {neg_sample}
                ---
                """
                summary = call_llm(prompt, st.session_state.selected_model)
                st.markdown(summary)

# --- Tab 5: å½±è¯„æ™ºèƒ½å‰–æ ---
with tab5:
    st.header("ğŸ¤– å½±è¯„æ™ºèƒ½å‰–æ")
    st.markdown("#### 1. åŸºäºæœ¬åœ°æ¨¡å‹çš„ç®€å•æƒ…æ„Ÿåˆ†æ")
    st.markdown("è¾“å…¥ä¸€æ®µå½±è¯„ï¼Œåœ¨IMDBæ•°æ®é›†ä¸Šå¾®è°ƒçš„distilbertæ¨¡å‹å°†è¿›è¡Œæƒ…æ„Ÿåˆ¤æ–­ã€‚")
    with st.spinner("åŠ è½½æƒ…æ„Ÿåˆ†ææ¨¡å‹..."):
        sentiment_pipeline = load_sentiment_pipeline()

    user_input_simple = st.text_area("åœ¨æ­¤è¾“å…¥è‹±æ–‡å½±è¯„æ–‡æœ¬(è®­ç»ƒé›†ä¸ºè‹±æ–‡æ–‡æœ¬):",
                              "This movie was not just bad, it was a catastrophe. The acting felt wooden and the plot was full of holes.",
                              height=150, key="simple_analysis_input")

    if st.button("è¿›è¡Œç®€å•æƒ…æ„Ÿåˆ†æ"):
        if user_input_simple:
            with st.spinner("æœ¬åœ°æ¨¡å‹åˆ†æä¸­..."):
                result = sentiment_pipeline(user_input_simple)[0]
                if result['label'].startswith('POSITIVE') or result['label'] == 'LABEL_1':
                    st.success(f"åˆ†æç»“æœï¼šæ­£é¢æƒ…æ„Ÿ (ç½®ä¿¡åº¦: {result['score']:.2%})")
                else:
                    st.error(f"åˆ†æç»“æœï¼šè´Ÿé¢æƒ…æ„Ÿ (ç½®ä¿¡åº¦: {result['score']:.2%})")
        else:
            st.warning("è¯·è¾“å…¥æ–‡æœ¬åå†è¿›è¡Œåˆ†æã€‚")

    st.markdown("---")
    current_model_name_tab5 = st.session_state.selected_model.split(' ')[0]
    st.markdown(f"#### 2. åŸºäº {current_model_name_tab5} çš„å¤šç»´åº¦æ·±åº¦åˆ†æ")
    st.markdown(f"è¾“å…¥ä¸€æ®µå½±è¯„ï¼Œ**{current_model_name_tab5}** å°†ä¸ºæ‚¨æä¾›å¤šç»´åº¦çš„æ·±åº¦åˆ†ææŠ¥å‘Šï¼Œè€Œä¸ä»…ä»…æ˜¯ç®€å•çš„æƒ…æ„Ÿåˆ¤æ–­ã€‚")

    user_input_deep = st.text_area("åœ¨æ­¤è¾“å…¥å½±è¯„æ–‡æœ¬:",
                              "**è¿™éƒ¨ç”µå½±ç®€ç›´å¤ªæ£’äº†ï¼Œå ªç§°ä¸€åœºè§†è§‰ä¸å¿ƒçµçš„ç››å®´ï¼** è™½ç„¶å‰§æƒ…èŠ‚å¥ç¨æ…¢ï¼Œä½†æ¼”å‘˜çš„è¡¨æ¼”å ªç§°å®Œç¾ï¼Œæƒ…æ„ŸçœŸæŒšåŠ¨äººï¼Œæƒ…èŠ‚è®¾è®¡æ›´æ˜¯ç¯ç¯ç›¸æ‰£ã€å¼•äººå…¥èƒœã€‚æ¯ä¸€ä¸ªé•œå¤´éƒ½å……æ»¡è‰ºæœ¯æ„Ÿï¼Œé…ä¹æ›´æ˜¯ç‚¹ç›ä¹‹ç¬”ã€‚çœŸåº†å¹¸æ²¡æœ‰é”™è¿‡è¿™éƒ¨æ°ä½œï¼Œä¸¤ä¸ªå°æ—¶çš„è§‚å½±ä½“éªŒè®©äººæ„çŠ¹æœªå°½ï¼",
                              height=150, key="deep_analysis_input")

    if st.button("ğŸš€ å¼€å§‹æ·±åº¦åˆ†æ"):
        if user_input_deep:
            prompt = f"""
            ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å½±è¯„äººã€‚è¯·åˆ†æä»¥ä¸‹å½±è¯„ï¼Œå¹¶ä»¥ Markdown æ ¼å¼æä¾›ç»“æ„åŒ–çš„åˆ†ææŠ¥å‘Šã€‚
            ä½ çš„åˆ†ææŠ¥å‘Šåº”åŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š

            - **ğŸ“ æ€»ä½“æƒ…ç»ª**ï¼šè¯·æ¸…æ™°åœ°è¯´æ˜è¯¥å½±è¯„çš„è¯„ä»·æ˜¯â€œæ­£é¢â€ã€â€œè´Ÿé¢â€è¿˜æ˜¯â€œè¤’è´¬ä¸ä¸€â€ã€‚
            - **ğŸ­ å…³é”®æ–¹é¢åˆ†æ**ï¼šè¯·è¯†åˆ«å½±è¯„ä¸­æåˆ°çš„ç”µå½±çš„å…·ä½“æ–¹é¢ï¼ˆä¾‹å¦‚ï¼Œå‰§æƒ…ã€æ¼”æŠ€ã€æ‘„å½±ã€èŠ‚å¥ã€é…ä¹ï¼‰ã€‚å¯¹äºæ¯ä¸ªæ–¹é¢ï¼Œè¯·ç®€è¦æè¿°å½±è¯„äººçš„è§‚ç‚¹ï¼Œå¹¶ä¸ºå…¶åˆ†é…ç›¸åº”çš„æƒ…ç»ªï¼ˆâ€œæ­£é¢â€ã€â€œè´Ÿé¢â€ã€â€œä¸­æ€§â€ï¼‰ã€‚
            - **ğŸ’¡ æ ¸å¿ƒä¿¡æ¯**ï¼šè¯·ç”¨ä¸€å¥è¯æ¦‚æ‹¬å½±è¯„äººçš„ä¸»è¦è§‚ç‚¹ã€‚

            ä»¥ä¸‹æ˜¯éœ€è¦åˆ†æçš„å½±è¯„ï¼š
            ---
            "{user_input_deep}"
            ---
            """
            with st.spinner(f"ğŸ¤– {current_model_name_tab5} æ­£åœ¨è¿›è¡Œæ·±åº¦å‰–æ..."):
                analysis_result = call_llm(prompt, st.session_state.selected_model)
                st.markdown("---")
                st.subheader(f"ğŸ” {current_model_name_tab5} æ·±åº¦åˆ†ææŠ¥å‘Š")
                st.markdown(analysis_result)
        else:
            st.warning("è¯·è¾“å…¥æ–‡æœ¬åå†è¿›è¡Œåˆ†æã€‚")
